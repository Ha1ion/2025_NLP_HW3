{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ha1ion/2025_NLP_HW3/blob/main/nlp_hw3_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "此作業有使用Gemini幫忙下註解幫助批改"
      ],
      "metadata": {
        "id": "2yx5T8GwAKqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"datasets==2.18.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0WV2JvO_kZG",
        "outputId": "581f6224-6dfa-40fa-d104-d14d5059746c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.18.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.70.16)\n",
            "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.18.0 fsspec-2024.2.0 pyarrow-hotfix-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOrPV3dRBQeo",
        "outputId": "ee3e9d47-9036-4ea5-f642-8e3e2b2edab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (2.18.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#  You can install and import any other libraries if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "biBQQmrgBQet"
      },
      "outputs": [],
      "source": [
        "# Some Chinese punctuations will be tokenized as [UNK], so we replace them with English ones\n",
        "token_replacement = [\n",
        "    [\"：\" , \":\"],\n",
        "    [\"，\" , \",\"],\n",
        "    [\"“\" , \"\\\"\"],\n",
        "    [\"”\" , \"\\\"\"],\n",
        "    [\"？\" , \"?\"],\n",
        "    [\"……\" , \"...\"],\n",
        "    [\"！\" , \"!\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krju_GCVBQeu",
        "outputId": "a0942df5-9951-473c-d9c2-ff54283354fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"./cache/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWVnbU7OBQev"
      },
      "outputs": [],
      "source": [
        "class SemevalDataset(Dataset):\n",
        "    def __init__(self, split=\"train\") -> None:\n",
        "        super().__init__()\n",
        "        assert split in [\"train\", \"validation\", \"test\"]\n",
        "        self.data = load_dataset(\n",
        "            \"sem_eval_2014_task_1\", split=split, trust_remote_code=True, cache_dir=\"./cache/\"\n",
        "        ).to_list()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        d = self.data[index]\n",
        "        # Replace Chinese punctuations with English ones\n",
        "        for k in [\"premise\", \"hypothesis\"]:\n",
        "            for tok in token_replacement:\n",
        "                d[k] = d[k].replace(tok[0], tok[1])\n",
        "        return d\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
        "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bWfp27QCBQew"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters\n",
        "# You can modify these values if needed\n",
        "lr = 3e-5\n",
        "epochs = 3\n",
        "train_batch_size = 8\n",
        "validation_batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D8SGq6m-BQex"
      },
      "outputs": [],
      "source": [
        "# TODO1: Create batched data for DataLoader\n",
        "# `collate_fn` is a function that defines how the data batch should be packed.\n",
        "# This function will be called in the DataLoader to pack the data batch.\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # TODO1-1: Implement the collate_fn function\n",
        "\n",
        "    # 1. 從 batch 中分別取出所有 premise 和 hypothesis\n",
        "    premises = [d['premise'] for d in batch]\n",
        "    hypotheses = [d['hypothesis'] for d in batch]\n",
        "\n",
        "    # 2. 使用 tokenizer 處理句子對\n",
        "    inputs = tokenizer(\n",
        "        premises,\n",
        "        hypotheses,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # 3. 處理標籤\n",
        "    # [FIX]: 根據 print 輸出的真實 key\n",
        "\n",
        "    # Sub-task 1: 使用 'relatedness_score'\n",
        "    inputs['labels_sim'] = torch.tensor(\n",
        "        [d['relatedness_score'] for d in batch],\n",
        "        dtype=torch.float\n",
        "    )\n",
        "\n",
        "    # Sub-task 2: 使用 'entailment_judgment' (注意拼寫!)\n",
        "    inputs['labels_ent'] = torch.tensor(\n",
        "        [d['entailment_judgment'] for d in batch],\n",
        "        dtype=torch.long\n",
        "    )\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# TODO1-2: Define your DataLoader\n",
        "# (這部分的程式碼 dl_train, dl_validation, dl_test 保持不變)\n",
        "\n",
        "# 1. 建立 Dataset 實例\n",
        "train_dataset = SemevalDataset(split=\"train\")\n",
        "validation_dataset = SemevalDataset(split=\"validation\")\n",
        "test_dataset = SemevalDataset(split=\"test\")\n",
        "\n",
        "# 2. 建立 DataLoader\n",
        "dl_train = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "dl_validation = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=validation_batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "dl_test = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=validation_batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PU0mrJ8vBQey"
      },
      "outputs": [],
      "source": [
        "# TODO2: Construct your model\n",
        "class MultiLabelModel(torch.nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # Write your code here\n",
        "        # Define what modules you will use in the model\n",
        "        # Please use \"google-bert/bert-base-uncased\" model (https://huggingface.co/google-bert/bert-base-uncased)\n",
        "        # Besides the base model, you may design additional architectures by incorporating linear layers, activation functions, or other neural components.\n",
        "        # Remark: The use of any additional pretrained language models is not permitted.\n",
        "\n",
        "        # 1. 載入 BERT-base-uncased 模型\n",
        "        self.bert = BertModel.from_pretrained(\n",
        "            \"google-bert/bert-base-uncased\",\n",
        "            cache_dir=\"./cache/\"\n",
        "        )\n",
        "\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        # 2. 定義 Sub-task 1 (迴歸) 的輸出頭\n",
        "        # 輸出維度為 1 (預測 relatedness_score)\n",
        "        self.regression_head = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # 3. 定義 Sub-task 2 (分類) 的輸出頭\n",
        "        # 輸出維度為 3 (3 個類別: 0, 1, 2)\n",
        "        self.classification_head = torch.nn.Linear(hidden_size, 3)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        # Write your code here\n",
        "        # Forward pass\n",
        "\n",
        "        # 1. 將 collate_fn 傳來的 input_ids 和 attention_mask 傳入 BERT\n",
        "        # 我們從 **kwargs 中取出 'labels_sim' 和 'labels_ent'，這樣它們就不會被傳入 BERT\n",
        "        labels_sim = kwargs.pop(\"labels_sim\", None)\n",
        "        labels_ent = kwargs.pop(\"labels_ent\", None)\n",
        "\n",
        "        # **kwargs 現在只包含 BERT 接受的參數 (input_ids, attention_mask, token_type_ids)\n",
        "        bert_output = self.bert(**kwargs)\n",
        "\n",
        "        # 2. 取得 [CLS] token 的輸出 (pooler_output)\n",
        "        # 這是整個輸入序列 (premise + hypothesis) 的語意表示\n",
        "        pooled_output = bert_output.pooler_output\n",
        "\n",
        "        # 3. 將 pooled_output 分別傳入兩個 head\n",
        "        logits_sim = self.regression_head(pooled_output)\n",
        "        logits_ent = self.classification_head(pooled_output)\n",
        "\n",
        "        # 4. 回傳兩個 head 的輸出\n",
        "        return logits_sim, logits_ent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6HPa7yD1BQez"
      },
      "outputs": [],
      "source": [
        "# TODO3: Define your optimizer and loss function\n",
        "\n",
        "model = MultiLabelModel().to(device)\n",
        "# TODO3-1: Define your Optimizer\n",
        "# We use AdamW as recommended by the PDF  and it's standard for Transformers.\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# TODO3-2: Define your loss functions (you should have two) [cite: 171]\n",
        "# Use different loss functions for different types of tasks.\n",
        "\n",
        "# Sub-task 1 (relatedness_score) is regression, so we use MSELoss.\n",
        "loss_sim_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Sub-task 2 (entailment_judgement) is 3-class classification, so we use CrossEntropyLoss.\n",
        "loss_ent_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# scoring functions\n",
        "psr = load(\"pearsonr\")\n",
        "acc = load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = 0.0\n",
        "for ep in range(epochs):\n",
        "    pbar = tqdm(dl_train)\n",
        "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
        "    model.train()\n",
        "    # TODO4: Write the training loop\n",
        "    # Write your code here\n",
        "    # train your model\n",
        "    # clear gradient\n",
        "    # forward pass\n",
        "    # compute loss\n",
        "    # back-propagation\n",
        "    # model optimization\n",
        "\n",
        "    # 初始化 total loss 來追蹤這個 epoch 的平均 loss\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for batch in pbar:\n",
        "        # 1. 將資料移動到 device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # 2. 取得標籤\n",
        "        labels_sim = batch['labels_sim']\n",
        "        labels_ent = batch['labels_ent']\n",
        "\n",
        "        # 3. clear gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. forward pass\n",
        "        # 我們在模型 forward 中已經處理了 **kwargs，所以可以直接傳入 batch\n",
        "        logits_sim, logits_ent = model(**batch)\n",
        "\n",
        "        # 5. compute loss\n",
        "        # 迴歸 loss (記得 squeeze logits_sim 才能匹配 (batch_size,) 的 shape)\n",
        "        loss_sim = loss_sim_fn(logits_sim.squeeze(), labels_sim)\n",
        "        # 分類 loss\n",
        "        loss_ent = loss_ent_fn(logits_ent, labels_ent)\n",
        "\n",
        "        # 合併兩個 loss\n",
        "        total_loss = loss_sim + loss_ent\n",
        "\n",
        "        # 6. back-propagation\n",
        "        total_loss.backward()\n",
        "\n",
        "        # 7. model optimization\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += total_loss.item()\n",
        "        pbar.set_postfix({\"loss\": total_loss.item()})\n",
        "\n",
        "    print(f\"Epoch {ep+1} Average Train Loss: {total_train_loss / len(dl_train)}\")\n",
        "\n",
        "    pbar = tqdm(dl_validation)\n",
        "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
        "    model.eval()\n",
        "\n",
        "    # TODO5: Write the evaluation loop\n",
        "    # Write your code here\n",
        "    # Evaluate your model\n",
        "    # Output all the evaluation scores (PearsonCorr, Accuracy)\n",
        "\n",
        "    # 建立 list 來儲存所有預測和標籤\n",
        "    all_preds_sim = []\n",
        "    all_labels_sim = []\n",
        "    all_preds_ent = []\n",
        "    all_labels_ent = []\n",
        "\n",
        "    with torch.no_grad(): # 驗證時不需要計算梯度\n",
        "        for batch in pbar:\n",
        "            # 1. 將資料移動到 device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # 2. 取得標籤\n",
        "            labels_sim = batch['labels_sim']\n",
        "            labels_ent = batch['labels_ent']\n",
        "\n",
        "            # 3. forward pass\n",
        "            logits_sim, logits_ent = model(**batch)\n",
        "\n",
        "            # 4. 處理預測結果\n",
        "            # 迴歸預測 (squeeze)\n",
        "            preds_sim = logits_sim.squeeze()\n",
        "            # 分類預測 (argmax)\n",
        "            preds_ent = torch.argmax(logits_ent, dim=1)\n",
        "\n",
        "            # 5. 收集結果 (移回 CPU)\n",
        "            all_preds_sim.extend(preds_sim.cpu().tolist())\n",
        "            all_labels_sim.extend(labels_sim.cpu().tolist())\n",
        "            all_preds_ent.extend(preds_ent.cpu().tolist())\n",
        "            all_labels_ent.extend(labels_ent.cpu().tolist())\n",
        "\n",
        "    # 在迴圈結束後，計算整體分數\n",
        "\n",
        "    # PearsonCorr [cite: 252]\n",
        "    pearson_corr = psr.compute(\n",
        "        predictions=all_preds_sim,\n",
        "        references=all_labels_sim\n",
        "    )['pearsonr']\n",
        "\n",
        "    # Accuracy [cite: 253]\n",
        "    accuracy = acc.compute(\n",
        "        predictions=all_preds_ent,\n",
        "        references=all_labels_ent\n",
        "    )['accuracy']\n",
        "\n",
        "    print(f\"Epoch {ep+1} Validation:\")\n",
        "    print(f\"Pearson Correlation: {pearson_corr}\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "    # 儲存最佳模型\n",
        "    # (修正：範本中的 'best' 變數應為 'best_score')\n",
        "    current_score = pearson_corr + accuracy\n",
        "    if current_score > best_score:\n",
        "        best_score = current_score\n",
        "        print(f\"New best score: {best_score}. Saving model...\")\n",
        "        # 確保 saved_models 資料夾存在\n",
        "        import os\n",
        "        os.makedirs(\"./saved_models\", exist_ok=True)\n",
        "        torch.save(model.state_dict(), f'./saved_models/best_model.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYWO2lhoA5s6",
        "outputId": "c71730ad-b579-4930-bdd6-07317cc022f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [1/3]: 100%|██████████| 563/563 [00:51<00:00, 11.02it/s, loss=1.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Train Loss: 1.209146320899257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation epoch [1/3]: 100%|██████████| 63/63 [00:01<00:00, 50.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Validation:\n",
            "Pearson Correlation: 0.8569097345845805\n",
            "Accuracy: 0.846\n",
            "New best score: 1.7029097345845805. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [2/3]: 100%|██████████| 563/563 [00:50<00:00, 11.22it/s, loss=0.186]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Average Train Loss: 0.5650650981423274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation epoch [2/3]: 100%|██████████| 63/63 [00:01<00:00, 45.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Validation:\n",
            "Pearson Correlation: 0.8672353330777036\n",
            "Accuracy: 0.846\n",
            "New best score: 1.7132353330777037. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epoch [3/3]: 100%|██████████| 563/563 [00:50<00:00, 11.24it/s, loss=1.57]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Average Train Loss: 0.4220690166124442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation epoch [3/3]: 100%|██████████| 63/63 [00:01<00:00, 51.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Validation:\n",
            "Pearson Correlation: 0.8622920535556333\n",
            "Accuracy: 0.854\n",
            "New best score: 1.7162920535556334. Saving model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptp0WSGcBQe0",
        "outputId": "845e8eb0-5101-4c08-ceb1-b489d83444dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 616/616 [00:11<00:00, 53.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Set Results ---\n",
            "Final Pearson Correlation: 0.8718775041294899\n",
            "Final Accuracy: 0.8571138623909073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = MultiLabelModel().to(device)\n",
        "# 載入我們儲存的最佳模型權重\n",
        "model.load_state_dict(torch.load(f\"./saved_models/best_model.ckpt\", weights_only=True))\n",
        "\n",
        "# Test Loop\n",
        "pbar = tqdm(dl_test, desc=\"Test\")\n",
        "model.eval()\n",
        "\n",
        "# TODO6: Write the test loop\n",
        "# Write your code here\n",
        "# We have loaded the best model with the highest evaluation score for you\n",
        "# Please implement the test loop to evaluate the model on the test dataset\n",
        "# We will have 10% of the total score for the test accuracy and pearson correlation\n",
        "\n",
        "# 建立 list 來儲存所有預測和標籤\n",
        "all_preds_sim = []\n",
        "all_labels_sim = []\n",
        "all_preds_ent = []\n",
        "all_labels_ent = []\n",
        "\n",
        "with torch.no_grad(): # 測試時不需要計算梯度\n",
        "    for batch in pbar:\n",
        "        # 1. 將資料移動到 device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # 2. 取得標籤\n",
        "        labels_sim = batch['labels_sim']\n",
        "        labels_ent = batch['labels_ent']\n",
        "\n",
        "        # 3. forward pass\n",
        "        logits_sim, logits_ent = model(**batch)\n",
        "\n",
        "        # 4. 處理預測結果\n",
        "        preds_sim = logits_sim.squeeze()\n",
        "        preds_ent = torch.argmax(logits_ent, dim=1)\n",
        "\n",
        "        # 5. 收集結果 (移回 CPU)\n",
        "        all_preds_sim.extend(preds_sim.cpu().tolist())\n",
        "        all_labels_sim.extend(labels_sim.cpu().tolist())\n",
        "        all_preds_ent.extend(preds_ent.cpu().tolist())\n",
        "        all_labels_ent.extend(labels_ent.cpu().tolist())\n",
        "\n",
        "# 在迴圈結束後，計算並印出最終的測試分數\n",
        "\n",
        "# PearsonCorr\n",
        "test_pearson_corr = psr.compute(\n",
        "    predictions=all_preds_sim,\n",
        "    references=all_labels_sim\n",
        ")['pearsonr']\n",
        "\n",
        "# Accuracy\n",
        "test_accuracy = acc.compute(\n",
        "    predictions=all_preds_ent,\n",
        "    references=all_labels_ent\n",
        ")['accuracy']\n",
        "\n",
        "print(\"\\n--- Test Set Results ---\")\n",
        "print(f\"Final Pearson Correlation: {test_pearson_corr}\")\n",
        "print(f\"Final Accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}